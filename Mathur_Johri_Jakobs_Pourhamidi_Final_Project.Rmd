---
title: "Mathur_Johri_Jakobs_Pourhamidi_Final_Project"
author: "Abeer Mathur, Marvin Jakobs, Divyesh Johri, Cinah Pourhamidi"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
library(dplyr)
library(tidyverse)
library(readr)
library(rpart)
library(randomForest)
library(stats)
library(FNN)
library(fastDummies)
library(kernlab)
```

#### Load and clean datasets
```{r message=FALSE, warning=FALSE}
CODGameModes <- read.csv("https://raw.githubusercontent.com/Abeermathur7/STAT380Final/main/CODGameModes.csv")
CODGames_p1_380 <- read.csv("https://raw.githubusercontent.com/Abeermathur7/STAT380Final/main/CODGames_p1_380.csv")
CODGames_p2_380 <- read.csv("https://raw.githubusercontent.com/Abeermathur7/STAT380Final/main/CODGames_p2_380.csv")

CODGames<- rbind(CODGames_p1_380,CODGames_p2_380) #combine raw data

dupModes <- CODGameModes
dupModes$Mode <- paste("HC -", dupModes$Mode) #adding HC mode limits
allModes <- rbind(dupModes,CODGameModes)
```

### Task 1 (Exploratory Analysis): 

#### Research Question: Which game mode is most likely to reach the score limit? 
```{r message=FALSE, warning=FALSE}
scoreLimit <- left_join(CODGames,allModes, by = c("GameType"="Mode")) #add score limit column

scoreLimit <- tidyr::separate(data = scoreLimit, col = Result, sep = "-", into = c("score1","score2")) #put each team score into individual columns

scoreLimit <- mutate(scoreLimit, limitReached = ifelse(score1 == ScoreLimit | score2 == ScoreLimit,1,0)) #note if either team reached the limit
         
limitList <- scoreLimit %>% group_by(GameType) %>% tally(limitReached) #group by and count games that reached score limit for each mode
totalCount <- scoreLimit %>% group_by(GameType) %>% summarise(count = n()) #total games of each mode played
limitList$total <- totalCount$count 
limitList <- mutate(limitList, limitProb = n/total) #find ratio of score limit reached to games played
limitList %>% arrange(desc(limitProb)) #print descending
```
```{r}
ggplot(limitList, aes(x=GameType,y=limitProb)) + 
  geom_bar(stat = "identity", fill = "#301934") + 
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  ylab("Prob of Score Limit Reached")
```

Both types of Domination and Hardpoint have the highest chances of reaching the score limit, with domination reaching it 100% of the time. This may be because...............

### Task 2 (Inference):

#### Research Question: Which predictors are associated with the TotalXP?
```{r}
subCODGames <- CODGames %>% select(1:16) #remove cols where almost every row is NA
model <- lm(TotalXP ~ ., data = subCODGames) #build the linear model
step_result <- step(model, direction = "backward") # backward-elimination for vars with best p-value
```

To find which predictors are associated with the TotalXP the most we preformed a linear regression model on all variables besides TotalXP then preformed backwards stepwise variable selection to find and rank the best predicting variables, some of which include Eliminations, FullPartial, Damage, and Deaths We can then see that the variable with the lowest and best AIC is Eliminations, which is what we will use in the next part.

#### Research Question: Of the predictors associated with the response, select one of the predictors and explain the relationship between the predictor and TotalXP. 
```{r message=FALSE, warning=FALSE}
groupElim <- CODGames %>% group_by(Eliminations) %>% summarize(avgXP = mean(TotalXP)) #group by num of eliminations and average the TotalXP
elim.reg <- lm(avgXP ~ Eliminations, data = groupElim) #create the lm to report the slope/intercept later
string <- paste("XP =",round(elim.reg$coefficients[[2]],2),"Elims +",round(elim.reg$coefficients[[1]],2)) #turn lm coeff into a formula string
ggplot(groupElim, aes(x=Eliminations, y=avgXP)) + geom_point() +  #ggplot with method lm and the formula string created above
  geom_smooth(method = "lm") + annotate("text",x = 10, y = 40000, label = string,size = 4) 
```

Of the best predictors found with backwards stepwise selection we picked the best one, 'Eliminations', which has a very strong positive linear correlation to the TotalXP variable. We calculated this by grouping the number of eliminations into a new dataframe and averaging the TotalXP received by people getting that number of eliminations into a new column of that dataframe. We then used ggplot to observe a very strong linear relationship before adding the linear regression model's line of best fit and printing the slope and intercept. We can interpret from the slope and intercept coefficient that for each elimination one gains on average 518.82 XP and with no eliminations one still averages 5807.79 XP


```{r message=FALSE, warning=FALSE}
# Cleaning the data from NAs
subCODGames <- subCODGames %>% drop_na()

# Function to help separate Result into a categorical variable
matchResult <- function(x){
  results <- str_split(x, "-")[[1]]
  diff <- strtoi(results[1]) - strtoi(results[2])
  if(diff < 0){
    return("Loss")
  } else if (diff > 0) {
    return("Win")
  } else {
    return("Draw")
  }
}
# Separating Result into a categorical variable
subCODGames$ResultCat <- apply(data.frame(subCODGames$Result), 1, matchResult)

# Make PrimaryWeapon a factor
subCODGames$PrimaryWeapon <- as.factor(subCODGames$PrimaryWeapon)

# Remove spaces from categorical variable values
cat_vars = c('Map1', 'Map2', 'Choice', 'FullPartial', 'ResultCat', 'XPType', 'DidPlayerVote', 'GameType')
for(cat_var in cat_vars){
  subCODGames[,cat_var] <- apply(data.frame(subCODGames[,cat_var]), 1, str_replace_all, pattern=" ", replacement="_")
  subCODGames[,cat_var] <- apply(data.frame(subCODGames[,cat_var]), 1, str_replace_all, pattern="-", replacement="_")
  subCODGames[,cat_var] <- apply(data.frame(subCODGames[,cat_var]), 1, str_replace_all, pattern="'", replacement="_")
  subCODGames[,cat_var] <- apply(data.frame(subCODGames[,cat_var]), 1, str_replace_all, pattern="%", replacement="percent")
  subCODGames[,cat_var] <- apply(data.frame(subCODGames[,cat_var]), 1, str_replace_all, pattern="\\+", replacement="plus")
}

# Make dummy variables (indicators) for each categorical variable
subCODGames_dummy <- dummy_cols(subCODGames, select_columns = cat_vars, remove_selected_columns = TRUE)


# Deselect variables that aren't useful
badVars <- c("Date", "Result", "MapVote")
subCODGames_dummy <- subCODGames_dummy %>% select(-badVars)
```

```{r}
# Select x variables for model building purposes
xvars <- names(select(subCODGames_dummy, -PrimaryWeapon))
# Scale the x variables (mainly for the kNN model)
subCODGames_dummy[,xvars] <- scale(subCODGames_dummy[,xvars], center = TRUE, scale = TRUE)

set.seed(123)
inds <- sample(1:nrow(subCODGames_dummy), floor(.8*nrow(subCODGames_dummy)))
Train <- subCODGames_dummy[inds, ]
Test <- subCODGames_dummy [-inds, ]
set.seed(NULL)
```

```{r}
set.seed(123)
model_rf <- randomForest(PrimaryWeapon ~ ., data = Train, ntrees = 500)
set.seed(NULL)

predictsRf <- predict(model_rf, newdata = Test)

cat("Accuracy Random Forest:", mean(predictsRf == Test$PrimaryWeapon))
```

```{r}
accvec <- rep(NA,50)

for (x in 2:50) {
  knn_res <- knn(train = Train[,xvars,drop=FALSE], 
               test = Test[,xvars,drop=FALSE], 
               cl=Train$PrimaryWeapon,
               k=x)
  
  accvec[x] <-  mean(Test$PrimaryWeapon == as.character(knn_res))
}

# Store results as a dataframe
k_compare <-  data.frame(k = 2:50, Acc=accvec[-1])

# Display a graph of the results to visualize the best k
ggplot(data=k_compare, aes(x=k,y=Acc)) +
  geom_line()
```

```{r message=FALSE, warning=FALSE}
cat("Accuracy KNN:",k_compare$Acc[33-1])
```

```{r message=FALSE, warning=FALSE}

modelSVM <- ksvm(PrimaryWeapon ~ ., data = Train)

predictions <- predict(modelSVM, newdata = Test)

contingency_table <- table(predictions, Test$PrimaryWeapon)

accuracy <- sum(diag(contingency_table)) / sum(contingency_table)

cat("Accuracy SVM:",accuracy)

```



